{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b846b4-0e3c-4d3a-a54e-b2b9a33cfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df08991-ef8c-47b5-927a-e8a1eb8c1b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data =  pd.read_csv(r\"C:\\Users\\Keith Martins\\OneDrive\\Documents\\PYTHON\\FIVERR PROJECTS\\House price prediction\\train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad9bc4d3-489e-4019-b15e-11e5e83e0405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spam_detector(train_df, valid_df, test_df):\n",
    "    \n",
    "    X = train_df.drop('label', axis='columns')\n",
    "    y = train_df['label']\n",
    "    \n",
    "    X_valid = valid_df.drop('label', axis='columns')\n",
    "    y_valid = valid_df['label']\n",
    "    \n",
    "    X_test = test_df.drop('label', axis='columns')\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    \n",
    "    word_pool = train_df.split(' ') # Split the words using space\n",
    "    words_unique = set(word_pool) #Obtain unique words\n",
    "    vectorizer = TfidfVectorizer() #initializing the vectorizer\n",
    "    vector_data = vectorizer.fit_transform(words_unique) #Applying TfidfVectorizer on the data\n",
    "\n",
    "    models = [ tree.DecisionTreeClassifier(), LogisticRegression(random_state = 1), MultinomialNB(random_state = 1), svm.SVC(kernel='linear') ]\n",
    "    \n",
    "    precision_scores =[]\n",
    "    for model in models:\n",
    "        model.fit(X,y)\n",
    "        y_pred = model.predict(test_data)\n",
    "        print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\", valid_cm)\n",
    "        print(\"Confusion Matrix:\", valid_cm)\n",
    "        precision_score = tp / (tp + fp )\n",
    "        print(\"precision score:\", precision_score)\n",
    "        precision_scores.append(model[i], precision_score,)\n",
    "     \n",
    "    model_precsion_score = {\n",
    "        \"DecisionTreeClassifier\": precision_scores[0],\n",
    "        \"LogisticRegression\": precision_scores[1],\n",
    "        \"MultinomialNB\": precision_scores[2],\n",
    "        \"LinearSVC\": precision_scores[3],\n",
    "    }\n",
    "    \n",
    "    confusion_matrices = {\n",
    "        \"DecisionTreeClassifier\": precision_scores[0],\n",
    "        \"LogisticRegression\": precision_scores[1],\n",
    "        \"MultinomialNB\": precision_scores[2],\n",
    "        \"LinearSVC\": precision_scores[3],\n",
    "    }\n",
    "    \n",
    "    best_classifier = max(model_precsion_score, key=model_precsion_score.get)\n",
    " \n",
    "\n",
    "    results = {\n",
    "        \"LogisticRegression\": precision_scores[1],\n",
    "        \"MultinomialNB\": precision_scores[2],\n",
    "        \"DecisionTreeClassifier\": precision_scores[0],\n",
    "        \"LinearSVC\": precision_scores[3],\n",
    "        \"BestClassifier\": best_classifier,\n",
    "        \"TfidfVectorizer\": vector_data,\n",
    "        \"Prediction\": None,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b67091b-af62-4805-83eb-7d15015fe812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_detector(train_df, valid_df, test_df):\n",
    "    \n",
    "    X = train_df.drop('label', axis='columns')\n",
    "    y = train_df['label']\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_pool = train_df.split(' ') # Split the words using space\n",
    "    words_unique = set(word_pool) #Obtain unique words\n",
    "    vectorizer = TfidfVectorizer() #initializing the vectorizer\n",
    "    vector_data = vectorizer.fit_transform(words_unique) #Applying TfidfVectorizer on the data\n",
    "\n",
    "    models = [ tree.DecisionTreeClassifier(), LogisticRegression(random_state = 1), MultinomialNB(random_state = 1), svm.SVC(kernel='linear') ]\n",
    "    \n",
    "    precision_scores =[]\n",
    "    for model in models:\n",
    "        model.fit(X,y)\n",
    "        y_pred = model.predict(test_data)\n",
    "        print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\", valid_cm)\n",
    "        print(\"Confusion Matrix:\", valid_cm)\n",
    "        precision_score = tp / (tp + fp )\n",
    "        print(\"precision score:\", precision_score)\n",
    "        precision_scores.append(model[i], precision_score,)\n",
    "     \n",
    "    model_precsion_score = {\n",
    "        \"DecisionTreeClassifier\": precision_scores[0],\n",
    "        \"LogisticRegression\": precision_scores[1],\n",
    "        \"MultinomialNB\": precision_scores[2],\n",
    "        \"LinearSVC\": precision_scores[3],\n",
    "    }\n",
    "    \n",
    "    confusion_matrices = {\n",
    "        \"DecisionTreeClassifier\": precision_scores[0],\n",
    "        \"LogisticRegression\": precision_scores[1],\n",
    "        \"MultinomialNB\": precision_scores[2],\n",
    "        \"LinearSVC\": precision_scores[3],\n",
    "    }\n",
    "    \n",
    "    best_classifier = max(model_precsion_score, key=model_precsion_score.get)\n",
    " \n",
    "\n",
    "    results = {\n",
    "        \"LogisticRegression\": precision_scores[1],\n",
    "        \"MultinomialNB\": precision_scores[2],\n",
    "        \"DecisionTreeClassifier\": precision_scores[0],\n",
    "        \"LinearSVC\": precision_scores[3],\n",
    "        \"BestClassifier\": best_classifier,\n",
    "        \"TfidfVectorizer\": vector_data,\n",
    "        \"Prediction\": None,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4859b19-5fba-4ff8-89d1-86c62a10b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"LightGBM is a gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage. It uses two novel techniques: Gradient-based One Side Sampling and Exclusive Feature Bundling (EFB) which fulfills the limitations of histogram-based algorithm that is primarily used in all GBDT (Gradient Boosting Decision Tree) frameworks. The two techniques of GOSS and EFB described below form the characteristics of LightGBM Algorithm. They comprise together to make the model work efficiently and provide it a cutting edge over other GBDT frameworks Gradient-based One Side Sampling Technique for LightGBM: Different data instances have varied roles in the computation of information gain. The instances with larger gradients(i.e., under-trained instances) will contribute more to the information gain. GOSS keeps those instances with large gradients (e.g., larger than a predefined threshold, or among the top percentiles), and only randomly drop those instances with small gradients  to retain the accuracy of information gain estimation. This  treatment can lead to a more accurate gain estimation than uniformly random sampling, with the same target sampling rate, especially when the value of information gain has a large range. \n",
    "           \"]\n",
    "\n",
    "def spam_detector(train_df, valid_df, test_df):\n",
    "    \n",
    "    \n",
    "    #Compute TFIFD\n",
    "    word_pool = train_df.split(' ') # Split the words using space\n",
    "    words_unique = set(word_pool) #Obtain unique words\n",
    "    vectorizer = TfidfVectorizer() #initializing the vectorizer\n",
    "    vector_data = vectorizer.fit_transform(words_unique) #Applying TfidfVectorizer on the data\n",
    "    \n",
    "    #EXTRACT FEATURE AND TARGET VARIABLES \n",
    "    X_train = train_df.drop('label', axis='columns')\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    \n",
    "    models = [ tree.DecisionTreeClassifier(), LogisticRegression(random_state = 1), MultinomialNB(random_state = 1), svm.SVC(kernel='linear') ]\n",
    "    \n",
    "    precision_scores = []\n",
    "\n",
    "    datasets = [valid_df, test_df]\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for model in models:\n",
    "        for dataset in datasets:         \n",
    "            y_d = dataset['label']\n",
    "            model.fit(X_train,y_train)\n",
    "            y_pred = model.predict(y_d)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_d, y_pred)\n",
    "            cm = (tn, fp, fn, tp)         \n",
    "            print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "            print(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbb731-3596-4a6a-a8e1-e060eefa3743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
